{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Function\n",
    "import torch.nn.functional as F\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleSoftmax(Function):\n",
    "    def forward(ctx, x):\n",
    "        exp_x = torch.exp(x - x.max(dim=-1, keepdim=True).values)\n",
    "        sum_exp = exp_x.sum(dim=-1, keepdim=True)\n",
    "        result = exp_x / sum_exp\n",
    "        ctx.save_for_backward(result)\n",
    "        return result\n",
    "\n",
    "    def backward(ctx, grad_output):\n",
    "        result, = ctx.saved_tensors\n",
    "        total_effect = (grad_output * result).sum(dim=-1, keepdim=True)\n",
    "        grad_input = result * (grad_output - total_effect)\n",
    "        return grad_input\n",
    "\n",
    "def my_simple_softmax(x):\n",
    "    return SimpleSoftmax.apply(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8923,  0.7547,  0.5274],\n",
      "        [ 1.3074, -0.0950,  0.9085],\n",
      "        [-0.6985,  0.1870,  1.0975],\n",
      "        [-0.7084,  2.2288, -0.2792]], requires_grad=True)\n",
      "\n",
      "Mine:\n",
      " tensor([[0.0968, 0.5027, 0.4005],\n",
      "        [0.5216, 0.1283, 0.3501],\n",
      "        [0.1058, 0.2565, 0.6376],\n",
      "        [0.0467, 0.8815, 0.0718]], grad_fn=<SimpleSoftmaxBackward>)\n",
      "\n",
      "Official:\n",
      " tensor([[0.0968, 0.5027, 0.4005],\n",
      "        [0.5216, 0.1283, 0.3501],\n",
      "        [0.1058, 0.2565, 0.6376],\n",
      "        [0.0467, 0.8815, 0.0718]], grad_fn=<SoftmaxBackward0>)\n",
      "\n",
      "Tensor close? True\n",
      "\n",
      "My gradient:\n",
      " tensor([[-0.0243,  0.0157,  0.0087],\n",
      "        [ 0.0507,  0.0083, -0.0590],\n",
      "        [ 0.0034, -0.0531,  0.0497],\n",
      "        [-0.0023,  0.0236, -0.0213]])\n",
      "\n",
      "Official gradient:\n",
      " tensor([[-0.0243,  0.0157,  0.0087],\n",
      "        [ 0.0507,  0.0083, -0.0590],\n",
      "        [ 0.0034, -0.0531,  0.0497],\n",
      "        [-0.0023,  0.0236, -0.0213]])\n",
      "\n",
      "Gradient close? True\n"
     ]
    }
   ],
   "source": [
    "#与库函数对比一下\n",
    "x = torch.randn(4, 3, requires_grad=True)\n",
    "print(x)\n",
    "\n",
    "labels = torch.tensor([0, 2, 1, 2])\n",
    "\n",
    "y_mine = my_simple_softmax(x)\n",
    "print(\"\\nMine:\\n\", y_mine)\n",
    "\n",
    "y_official = F.softmax(x, dim=-1)\n",
    "print(\"\\nOfficial:\\n\", y_official)\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"\\nTensor close?\", \n",
    "      torch.allclose(y_mine, y_official, atol=1e-6))\n",
    "\n",
    "loss_mine = loss(y_mine, labels)\n",
    "loss_official = loss(y_official, labels)\n",
    "\n",
    "loss_mine.backward()\n",
    "grad_mine = x.grad.clone()\n",
    "\n",
    "x.grad.zero_()\n",
    "loss_official.backward()\n",
    "grad_official = x.grad\n",
    "\n",
    "print(\"\\nMy gradient:\\n\", grad_mine)\n",
    "print(\"\\nOfficial gradient:\\n\", grad_official)\n",
    "\n",
    "# 比较反向传播梯度\n",
    "print(\"\\nGradient close?\",\n",
    "      torch.allclose(grad_mine, grad_official, atol=1e-6))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
