{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Function\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleSoftmax(Function):\n",
    "    def forward(ctx, x):\n",
    "        exp_x = torch.exp(x - x.max(dim=-1, keepdim=True).values)\n",
    "        sum_exp = exp_x.sum(dim=-1, keepdim=True)\n",
    "        result = exp_x / sum_exp\n",
    "        ctx.save_for_backward(result)\n",
    "        return result\n",
    "\n",
    "    def backward(ctx, grad_output):\n",
    "        result, = ctx.saved_tensors\n",
    "        total_effect = (grad_output * result).sum(dim=-1, keepdim=True)\n",
    "        grad_input = result * (grad_output - total_effect)\n",
    "        return grad_input\n",
    "\n",
    "def my_simple_softmax(x):\n",
    "    return SimpleSoftmax.apply(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5753,  0.1906, -1.5559],\n",
      "        [-0.0312,  0.7530,  0.3748]], requires_grad=True)\n",
      "\n",
      "Mine:\n",
      " tensor([[0.5557, 0.3783, 0.0660],\n",
      "        [0.2132, 0.4670, 0.3199]], grad_fn=<SimpleSoftmaxBackward>)\n",
      "\n",
      "Official:\n",
      " tensor([[0.5557, 0.3783, 0.0660],\n",
      "        [0.2132, 0.4670, 0.3199]], grad_fn=<SoftmaxBackward0>)\n",
      "\n",
      "Tensor close? True\n",
      "\n",
      "My gradient:\n",
      " tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [1.2705e-08, 2.7832e-08, 1.9067e-08]])\n",
      "\n",
      "Official gradient:\n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "\n",
      "Gradient close? True\n"
     ]
    }
   ],
   "source": [
    "#与库函数对比一下\n",
    "x = torch.randn(2, 3, requires_grad=True)\n",
    "print(x)\n",
    "\n",
    "y_mine = my_simple_softmax(x)\n",
    "print(\"\\nMine:\\n\", y_mine)\n",
    "\n",
    "y_official = F.softmax(x, dim=-1)\n",
    "print(\"\\nOfficial:\\n\", y_official)\n",
    "\n",
    "print(\"\\nTensor close?\", \n",
    "      torch.allclose(y_mine, y_official, atol=1e-6))\n",
    "\n",
    "loss_mine = y_mine.sum()\n",
    "loss_official = y_official.sum()\n",
    "\n",
    "loss_mine.backward()\n",
    "grad_mine = x.grad.clone()\n",
    "\n",
    "x.grad.zero_()\n",
    "loss_official.backward()\n",
    "grad_official = x.grad\n",
    "\n",
    "print(\"\\nMy gradient:\\n\", grad_mine)\n",
    "print(\"\\nOfficial gradient:\\n\", grad_official)\n",
    "\n",
    "# 比较反向传播梯度\n",
    "print(\"\\nGradient close?\",\n",
    "      torch.allclose(grad_mine, grad_official, atol=1e-6))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
